\subsection{Решение линейных систем алгебраических уравнений \label{methods:linear_solvers}}
Для метода Ньютона нам необходимо решать системы линейных алгебраических уравнений (СЛАУ)
общего вида:
\begin{equation} \label{eq:linear_system}
\mathbf{Ax} = \mathbf{b}
\end{equation}
Разделяют \textit{прямой} и  \textit{итерационный} подход к решению. \textit{Прямые} методы позволяют получить точное решение с погрешностью только от округления чисел с плавающей запятой. Количество операций в таких методов быстро растет с увеличением размера системы. Поэтому их применение для больших систем может быть неоправданно долгим. \textit{Итерационные} методы образуют последовательность приближенных решений, которая сходится к точному. Итерации можно остановить, если достигнута требуемая точность решения. Рассмотрим сначала \textit{прямые} методы, использованные в данной работе, потом \textit{итерационные}. В конце рассмотрим подробно методы \textit{подпространства Крылова} - особый класс методов.
\paragraph{Подпространством Крылова} называется линейное пространство
\begin{equation} \label{eq:krylov_subspace}
K_m(v, \mathbf{A}) = span\left\{ v, \mathbf{A}v, \mathbf{A}^2v, \dots, \mathbf{A}^{m-1}v \right\}
\end{equation}
Принцип действия рассматриваемых методов был впервые предложен в работе \cite{Krylov}.
Методы строят решение в \textit{подпространстве Крылова}. Одно из преимуществ таких методов - они позволяют не хранить численное представление матрицы $\mathbf{A}$. Достаточно знать, как она действует на произвольный вектор. Этот подход называется \textit{безматричным}. Методы подпространства Крылова могут быть как итерационными, так и прямыми.

\subsubsection{Алгоритм прогонки \label{methods:tridiagonal}}
Данный метод прямой. Он применяется, если матрица $\mathbf{A}$ имеет тридиагональный вид:
\begin{equation} \label{mat:tridiagonal}
\left[
\begin{array}{ccccccccc}
\times & \times & 0 & \ddots \\
\times & \times  & \times & 0 & \ddots \\
0 & \times & \times & \times & 0 & \ddots \\
\ddots & 0 & \times & \times & \times & 0 & \ddots \\
& \ddots & 0 & \times & \times & \times & 0 & \ddots \\
&& \ddots & 0 & \times & \times & \times & 0 \\
&&& \ddots & 0 & \times & \times & \times \\
&&&& \ddots & 0 & \times & \times\\
\end{array}
\right]
\end{equation}
где $\times$ - ненулевой элемент.
Такой вид имеет, например, матрица Якоби в одномерной задаче теплопроводности \eqref{eq:heat_equation_intro}.
В системе с тридиагональной матрицей неизвестные компоненты вектора $x$ связаны соотношениями:
\begin{equation}
A_i x_{i-1} + B_i x_i + C_i x_{i+1} = F_i
\end{equation}
По индукции можно показать, что справедливо:
\begin{equation}
x_i = \alpha_{i+1} x_{i+1} + \beta_{i+1}
\end{equation}
Можно выразить коэффициенты $\alpha_i$ и $\beta_i$ через $A_i, B_i, C_i$:
\begin{equation}
\begin{cases}
\alpha_2 = \frac{-C_1}{B_1} \\
\beta_2 = \frac{F_1}{B_1}
\end{cases}
\end{equation}
\begin{equation}
\begin{cases}
\alpha_{i+1} = \frac{-C_i}{A_i \alpha_i + B_I} \\
\beta_{i+1} = \frac{F_1 - A_i \beta_i}{A_i \alpha_i + B_i}
\end{cases}
\end{equation}
Нахождение этих коэффициентов равносильно преобразованию матрицы $\mathbf{A}$ к треугольному двудиагональному виду. Зная коэффициенты $\alpha_i$ и $\beta_i$ можно найти $x_i$. Метод прогонки позволяет найти решение за $O(M)$, где $M$ - количество элементов $x_i$. Метод рассматривается подробнее в \cite{Petrov}.

\subsubsection{Разложение Холецкого \label{methods:cholesky}}
Данный метод прямой. Если матрица симметрична и положительно определена, ее можно представить разложением Холецкого: $\mathbf{A} = \mathbf{LL}^T$ , где $\mathbf{L}$ - нижняя треугольная матрица. Систему линейных уравнений с треугольной матрицей можно эффективно решить. 
Найдя матрицу $L$, решение системы \eqref{eq:linear_system} сводится к решению 2-х систем с треугольными матрицами:
\begin{equation}
    \mathbf{L} \cdot \mathbf{y} = \mathbf{a};
    \quad
    \mathbf{L}^T \cdot \mathbf{x} = \mathbf{y}
\end{equation}
Алгоритм разложения приведен в \cite{Petrov}.

\subsubsection{Разложение LU \label{methods:lu}}
Данный метод прямой. Исходную матрицу можно разложить нижнюю $\mathbf{L}$ и верхнюю $\mathbf{U}$ треугольные матрицы. Метод применим к произвольной матрице, если она обратима, и все главные миноры невырождены. Получив матрицы $\mathbf{L}$ и $\mathbf{U}$, можно получить $\mathbf{x}$, решив 2 треугольные системы:
\begin{equation}
    \mathbf{L} \cdot \mathbf{y} = \mathbf{a};
    \quad
    \mathbf{U} \cdot \mathbf{x} = \mathbf{y}
\end{equation}
Алгоритм разложения приведен в \cite{Petrov}.

\subsubsection{Метод неполного разложения \label{methods:ilu}}
Применим \textit{разложение LU} (раздел \ref{methods:lu}) для разреженной матрицы. Результаты $\mathbf{L}$ и $\mathbf{U}$ не будут повторять шаблон разреженности исходной матрицы. Для ускорения расчета хотелось бы получить разреженные факторизации. Можно найти такое примерное разложение $\mathbf{A} \approx \mathbf{LU}$, где $\mathbf{L}$  и $\mathbf{U}$ - нижняя и верхняя тридиагональные матрицы, равные нулю там, где ${A}_{ij} = 0$. Для этого применяется такой же алгоритм, как для полного разложения. Однако вычисляются только те ячейки, где ${A}_{ij} \neq 0$, а остальные считаются равными $0$.  Это значительно ускоряет поиск факторизации для разреженной матрицы. Такой метод называют \textbf{ILU}.
\par
Можно изменить шаблон разреженности, например, сделать матрицы более полными. Это замедлит поиск факторизации, но сделает решение точнее. Золотая середина между скоростью и эффективностью подбирается экспериментально. Аналогичным образом можно получить разреженное разложение для \textit{метода Холецкого} (раздел \ref{methods:cholesky}). Более подробное описание дано в \cite{Golub}.

\subsubsection{Метод Якоби}
Данный метод итерационный. Пусть $\mathbf{D} = diag(\mathbf{A})$, $\mathbf{L}$ и $\mathbf{U}$ - нижняя и верхняя треугольные матрицы, $\mathbf{A} = \mathbf{D} + \mathbf{L} + \mathbf{U}$. Если $A_{ii} \neq 0$, систему \eqref{eq:linear_system} можно представить в виде $\mathbf{x} = \mathbf{Bx} + \mathbf{g}$, где $\mathbf{B} = \mathbf{D}^{-1} (\mathbf{D} - \mathbf{A})$. Тогда алгоритм поиска решения будет иметь вид:
\begin{equation}
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
\end{equation}
Метод рассматривается подробнее в \cite{Petrov}. Одним из главных преимуществ метода Якоби является его сглаживающее свойство, что позволяет его эффективно использовать в многосеточном методе \ref{methods:multigrid}.

\subsubsection{Метод Гаусса-Зейделя \label{methods:gauss_seidel}}
Данный метод итерационный. Он является модификацией \textit{метода Якоби}. Метод использует только что полученные значения, $x_i^{(k+1)}$ для нахождения $x_{i+1}^{(k+1)}$. Решаемая система выглядит таким образом:
\begin{equation}
(\mathbf{L} + \mathbf{D}) \mathbf{x}^{(k+1)} = -\mathbf{Ux}^{(k)} + \mathbf{b}
\end{equation}
Метод рассматривается подробнее в \cite{Petrov}. Данный метод также обладает сглаживающим свойством.

\subsubsection{Многосеточной метод (Multigrid) \label{methods:multigrid}}
Данный метод итерационный. Суть алгоритма состоит в решении задачи на разных сетках. Сетки последовательно становятся грубее, уменьшая размерность системы. Для интерполяции  результатов на более грубые сетки и назад заранее выбираются операторы интерполяции. На каждой сетке применяется выбранный итерационный метод решения системы линейных уравнений, например, \textit{метод Гаусса-Зейделя} (раздел \ref{methods:gauss_seidel}). Выбранный метод применяется не полностью, делаются только первые несколько итераций. Этот процесс называется \textit{сглаживанием}.
\paragraph{Алгоритм задается рекуррентно:}
\begin{itemize}
\item Сделать $k$ итераций сглаживания
\item Интерполировать $\mathbf{A}$ и $\mathbf{b}$ на более грубую сетку
\item Если мы не достигли самой грубой сетки: 
\begin{itemize}
\item Получить невязку $\mathbf{r}$ на более грубой сетке с помощью \textit{Многосеточного метода}
\item Интерполировать $\mathbf{r}$ на менее грубую сетку и вычесть ее 
\end{itemize}
\item Сделать $k$ итераций сглаживания
\end{itemize}
Число $k$ и количество грубых сеток выбираются заранее. 
\par
Преимущество данного метода заключается в том, что он действует на разные масштабы невязки, подавляя ее разные гармоники. Обычные итерационные методы плохо подавляют ''низкочастотную'' невязку. Количество операций линейно возрастает с увеличением размера задачи. 
\par
Возможен разный вид циклов \textit{Многосеточного метода}. Алгоритм, описанный выше, называется \textit{V-цикл}. Есть и другие известные вариации алгоритма - \textit{F-цикл} и \textit{W-цикл}. Структуры алгоритмов приведены на рис. \ref{fig:multigrid}. Разные циклы имеют разные свойства сходимости и применимы к разным задачам. Подробнее этот вопрос рассматривается в \cite{briggs2000multigrid}.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{common_images/MultigridWork.png}
\caption{Различные циклы Многосеточного метода. \begin{small}(Источник: \url{https://commons.wikimedia.org/wiki/File:MultigridWork.svg})\end{small}}
\label{fig:multigrid}
\end{figure}

\subsubsection{Метод сопряженных градиентов \label{methods:conjugate_gradients}}
\paragraph{Прямой подход}
Данный метод относится к методам подпространства Крылова.
Пусть $\mathbf{A}$ - симметричная, положительно-определенная матрица, а $\mathbf{x_*}$ - решение исходной системы. Будем приближаться к решению \textit{сопряженными} шагами. \textit{Сопряженными} относительно матрицы $\mathbf{A}$ называются такие векторы $\mathbf{u}$ и $\mathbf{v}$, что  $\mathbf{u^T A v} = 0$. Решение можно представить через набор сопряженных векторов $P = \{ \mathbf{p_1}, ..., \mathbf{p_n} \}$, они образуют базис подпространстве Крылова \eqref{eq:krylov_subspace}. Тогда искомое решение имеет вид:
\begin{equation}
\mathbf{x_*} = \sum^n_{i=1} \alpha_i \mathbf{p_i}
\end{equation}
Домножим его на $\mathbf{A}$:
\begin{equation}
\mathbf{A x_*} = \sum^n_{i=1} \alpha_i \mathbf{A p_i}
\end{equation}
Домножим на $\mathbf{p_k^T}$, пользуясь сопряженностью $\mathbf{p}$:
\begin{equation}
\mathbf{p}^T_k \mathbf{A} \mathbf{x_*} = \alpha_k \mathbf{p_k}^T \mathbf{A} \mathbf{p_k}
\end{equation}
Воспользуемся \eqref{eq:linear_system}, выразим $\alpha_k$:
\begin{equation}
\alpha_k = \frac{\mathbf{p}^T_k \mathbf{b}}{\mathbf{p_k}^T \mathbf{A} \mathbf{p_k}}
\end{equation}
Для точного решения нужно найти $n$ базисных векторов $\mathbf{p}_k$ и коэффициентов $\alpha_k$.

\paragraph{Итерационный подход}
Если удачно выбрать несколько первых базисных векторов $\mathbf{p}_k$, можно не делать все $n$ шагов
для получения решения с нужной точностью. Итерационный подход рассматривает решение системы как задачу
минимизации функции
\begin{equation}
f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T \mathbf{A} \mathbf{x} - \mathbf{x}^T \mathbf{b}
\end{equation}
где минимум $\mathbf{x_*}$ cуществует, поскольку вторая производная $\nabla ^2 f(\mathbf{x}) = \mathbf{A}$ симметрична и положительно определена. Градиентом функции является невязка исходной задачи:
\begin{equation}
\nabla f(\mathbf{x}) = \mathbf{r} = \mathbf{Ax} - \mathbf{b}
\end{equation}
Возьмем начальное приближение решения $\mathbf{x_0}$. Так как мы ищем минимум функции, можно взять первым базисным вектором отрицательный градиент в точке $\mathbf{x_0}$:
\begin{equation}
\mathbf{p}_0 = \mathbf{b} - \mathbf{Ax}_0
\end{equation}
Cледующие шаги выбираются в направлении, сопряженном ко всем предыдущим базисным векторам:
\begin{equation}
\mathbf{p}_k = \mathbf{r}_k + \sum_{i < k} \frac {\mathbf{p}^T_i \mathbf{A r}_k}
{\mathbf{p}^T_i \mathbf{A p}_i} \mathbf{p}_i
\end{equation}
где $\mathbf{r}_k = \mathbf{b} - \mathbf{Ax}_k$ - текущая невязка. Так находится приближенное решение
$\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k$.
\\ \\
Коэффициент $\alpha_k$ находится из минимизации $f(\mathbf{x}_{k+1})$ по параметру $\alpha_k$. Пользуемся $f(\mathbf{x}_{k+1}) = f(\mathbf{x}_k + \alpha_k \mathbf{p}_k) = g(\alpha_k)$, требуем $g'(\alpha) = 0$. Отсюда:
\begin{equation}
\alpha_k = \frac {\mathbf{p}^T_k (\mathbf{b} - \mathbf{A x}_k)} {\mathbf{p}^T_k \mathbf{A} \mathbf{p}_k}
\end{equation}
\par
Метод сопряженных градиентов может быть преобразован в форму, не требующую хранения в памяти всех базисных векторов $\mathbf{p}_k$. Сходимость метода оценивается в \cite{Golub}.
\par
Главный недостаток метода - матрица должна быть симметрична и положительно определена. В уравнении теплопроводности \eqref{eq:heat_equation_intro} эти требования выполняются. В системе \textit{функционала плотности} \eqref{eq:dhd_system_intro} они могут не выполняться. Поэтому необходимо рассмотреть другие методы, которые не накладывают такие ограничения.

\subsubsection{Метод обобщенных минимальных невязок (GMRES) \label{methods:gmres}}
Данный метод относится к методам подпространства Крылова.
Решение приближается через вектор в \textit{подпространстве Крылова} с минимальной невязкой. Подпространство строится по вектору невязки: $K_n = K_n(A, r_0)$, где $r_0 = b - Ax_0$, $x_0$ - начальное приближение. Точное решение приближается вектором $x_n \in K_n$, с которым невязка $r_n$ минимальна.
\par
Векторы $r_0, A r_0, \dots A^{n - 1} r_0 \in K_n$  могут быть практически линейно зависимыми. Удобнее построить ортонормированный базис $q_1,\dots q_n \in K_n$  с помощью \textit{итераций Арнольди} (процесс описан в \cite{saad2003iterative}). После этого решается задача минимизации нормы невязки и строится решение исходной системы. Базисные векторы $q_1, \dots q_n$  формируют матрицу $Q_n$, по которой раскладывается решение: $x_n = Q_n y_n$. В процессе \textit{итераций Арнольди} формируется верхняя матрица Хессенберга $\tilde H_n$ размерности $(n + 1) \times n$:

\begin{equation}
A Q_n = Q_{n+1} \tilde H_n
\end{equation}
Получим выражение нормы невязки для минимизации:
\begin{equation}
||r_n|| = ||A x_n - b|| = ||\tilde H_n y_n - Q^T_{n+1} b|| = ||\tilde H_n y_n - \beta e_1||
\end{equation}
Здесь мы воспользовались тем, что $Q_n$ образуется из ортонормированного базиса векторов. $e_1$ - первый базисный вектор в начальном базисе в $\mathbb{R}^{n+1}$. $\beta = ||b - A x_0||$. Таким образом, $x_n$ можно найти, минимизируя норму невязки:
\begin{equation}
r_n = \tilde H_n y_n - \beta e_1
\end{equation}
Данная задача сводится к задаче наименьших квадратов. Приведем алгоритм n-ой итерации метода GMRES:
\begin{enumerate}
\item Вычисление нового базисного вектора \textit{методом Арнольди}
\item Нахождение $y_n$, который минимизирует невязку $||r_n||$
\item Вычисление $x_n = Q_n y_n$
\item Повторение, если невязка недостаточно мала
\end{enumerate}

Метод GMRES может быть использован с произвольной обратимой матрицей. С увеличением размерности подпространства $K_n$ увеличивается объем используемой памяти. Поэтому раз в определенное количество итераций необходимо делать рестарт с новым приближенным решением.
\par
Подробное описание метода GMRES приводится в \cite{saad2003iterative}.

\subsubsection{Метод бисопряженных градиентов}
Метод сопряженных градиентов решает только системы с симметричной матрицей. Можно модифицировать его, чтобы снять это ограничение.
Данный метод ищет невязку в подпространстве Крылова \eqref{eq:krylov_subspace} $K(v, A)$, где вектор $v$ обычно берется нормированной невязкой начального приближения. Отличие от метода сопряженных градиентов заключается в том, что вектора проекций решения строятся ортогонально подпространству Крылова $K(w, A^T)$, где $w = b^* - A^T x_0^*$. Вектор $v$ должен быть ортогонален вектору $w$. Алгоритм метода приводится в \cite{saad2003iterative}.  

\subsubsection{Стабилизированный метод бисопряженных градиентов (BiCGStab) \label{methods:bicgstab}}
Данный метод разработан на основе \textit{метода бисопряженных градиентов} и \textit{метода GMRES}, чтобы улучшить стабильность сходимости. На каждом шагу бисопряженных градиентов решается система методом GMRES с рестартом на каждой итерации.
Подробнее данный метод описывается в \cite{saad2003iterative}.

\subsubsection{Предобуславливатели \label{methods:preconditioning}}
Оценки сходимости итерационных методов проведены в \cite{briggs2000multigrid}. Они показывают, что итерационные методы работают эффективно с хорошо обусловленной матрицей. Перед решением системы можно совершить линейное преобразование и решать \textit{предобусловленную} систему. 
\paragraph{Предобуславливателем} матрицы $\mathbf{A}$ называется такая матрица $\mathbf{M}$, что $\mathbf{M} \cdot \mathbf{A} \approx \mathbf{I}$, где $\mathbf{I}$ - единичная матрица. У $\mathbf{I}$ наилучшее число обусловленности: $\mu(\mathbf{I}) = 1$. Иначе говоря, $\mathbf{M} \approx \mathbf{A^{-1}}$. \textit{Алгоритмы подпространства Крылова} не используют саму матрицу, однако каждый из них можно модифицировать, чтобы решалась предобусловленная система $\mathbf{M} \cdot \mathbf{A} \cdot x = \mathbf{M} \cdot \mathbf{b}$ или $\mathbf{A} \cdot \mathbf{M} \cdot x = \mathbf{b} \cdot \mathbf{M} $.
\par
Для произвольного численного метода в качестве предобуславливателя можно эффективно использовать полную или неполную факторизацию Холецкого или LU (раздел \ref{methods:ilu}), поскольку они позволяют получить приближение обратной матрицы в явном виде.
В качестве предобуславливателя \textit{метода подпространства Крылова} можно использовать любой численный метод решения линейной системы. Достаточно сделать несколько итераций такого метода, чтобы примерное решение системы: $\mathbf{x} \approx \mathbf{A}^{-1} \mathbf{b}$. Подробнее использование предобуславливателей описано в \cite{Golub} и \cite{saad2003iterative}.